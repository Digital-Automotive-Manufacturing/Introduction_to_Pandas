{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "232add90-809e-4bc2-b5c8-e9e2a947479f",
   "metadata": {},
   "source": [
    "# Introduction to Python Pandas Assignment\n",
    "\n",
    "Please use the included F1 dataset in the GitRepo\n",
    "\n",
    "If needed, the F1 Dataset Download Link is: https://www.kaggle.com/datasets/dubradave/formula-1-drivers-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d467e-da2d-4a20-b91e-f624cce6e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Import the dataset into a DataFrame\n",
    "\n",
    "\n",
    "# TODO Print the first 15 lines of the dataset\n",
    "\n",
    "\n",
    "# TODO Print all of the column names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098e7e2b-3ae5-4bc7-976e-d8d4f45f11e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Store all data from Lewis Hamilton in a new DataFrame df_LH\n",
    "\n",
    "\n",
    "\n",
    "# TODO Print out df_LH\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e172a58-fadf-485f-bbd3-01aec36e8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Store all data from Brazilian drivers in a new DataFrame df_Brazil using .loc\n",
    "\n",
    "\n",
    "\n",
    "# TODO Print out df_Brazil\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ab817-8a30-41a7-8482-b03f47e97b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Using sorting and then .iloc (cannot use .head()), sort the drivers by \n",
    "# Points and then print only the drivers with the 3rd-10th most points\n",
    "\n",
    "\n",
    "print(df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8638faa-965d-4b4f-854a-9aa01a3aad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Using sorting and then iloc (cannot use .head()), sort the drivers by \n",
    "# Points and then print only the drivers with the 3rd, 5th, and 9th most points\n",
    "\n",
    "\n",
    "print(df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09183564-4a5e-402b-bfa2-e0953c5cb34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Using loc and conditional statements print out only drivers from Italy with either:\n",
    "# 50 or more points \n",
    "# or \n",
    "# more than 200 race entries\n",
    "\n",
    "\n",
    "print(df_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04a5dfd-f1f5-4981-99a0-c53cf12ec376",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## Data Exploration\n",
    "This is an individual assignment and bonus.\n",
    "\n",
    "For this next section, use the <b>F1 dataset</b> files from above. Complete an exploratory data analysis of the F1 dataset to understand and visualize it. \n",
    "\n",
    "Students should submit their code in the Python .ipynb. Each team needs to submit a “.zip” package including:\n",
    "- A “.pdf” file for the report of your JupyterLab Notebook with all explanation for your data exploration included. Be sure to include at least:\n",
    "    - What does the data represent?\n",
    "    - What are the important data statistics you found?\n",
    "    - What cleaning and preperation are needed or did you complete?\n",
    "    - What can you tell about the dataset so far?\n",
    "    - What might your next steps to model the data be? You do not need to model it at this point, only provide an inital plan.\n",
    "    - Did you find any interesting information about the dataset? Patterns, trends, or correlations?\n",
    "- A JupyterLab notebook including all code to execute the analysis. The code must run on it's own.\n",
    "\n",
    "### BONUS #1 (<b><ins>Due April 3, 2023 11:59 PM EST</ins></b>)\n",
    "- Bonus points up to 10 bonus HW points will be awarded to the group with the best and most creative data exploration. <ins>Think outside the box.</ins>\n",
    "    - 1st place = 10 pts\n",
    "    - 2nd place = 5 pts\n",
    "\n",
    "\n",
    "Resources for ideas:\n",
    "- https://www.epa.gov/caddis-vol4/exploratory-data-analysis\n",
    "- https://towardsdatascience.com/an-extensive-guide-to-exploratory-data-analysis-ddd99a03199e\n",
    "- https://www.simplilearn.com/tutorials/data-analytics-tutorial/exploratory-data-analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c6641-1e88-46dc-917b-d0687c0e33b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596939b5-1608-43b4-a8c5-4848e9cda1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5e87d2-7be8-4b67-b05e-3d820e5fba59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ab2ba5-0c5e-4bf5-a92e-9acf67f39179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47a9d1a6-cc92-4cec-b8ec-0948362bb28b",
   "metadata": {},
   "source": [
    "***\n",
    "### BONUS #2 (<b><ins>Due separately April 10, 2023 11:59 PM EST</ins></b>)\n",
    "This is a group bonus.\n",
    "\n",
    "For this section, use the <b>machine data files provided on Google Drive</b> at the link below (you must be logged in to your g.clemson.edu address to access) and the provided  and description PDF.\n",
    "\n",
    "The dataset can be downloaded here (~457 MB required): https://drive.google.com/drive/folders/1L_V7W1cv3doRBaQl7MHbrPg3X0BlTc6u?usp=share_link\n",
    "\n",
    "X, Y, and Z are the X, Y, and Z-axes respectively. gt is the Ground Truth file for the 1500 dataset.\n",
    "\n",
    "- The dataset includes vibration signals of normal and four different types of fault conditions and their corresponding ground-truth labels for two different operational conditions.\n",
    "- Vibration signals have been divided into smaller segments using window size of 200 data points to reduce the computational time.\n",
    "- Vibration signals are available in three directions (x, y, z). Participants are free to use either one direction or any combination of them.\n",
    "- The 1500_10 data will be used for training. You have x, y, and z data as well as the ground truth. \n",
    "- The 2700_25 data will be used for testing. You will only have the x, y, and z data but no ground truth.\n",
    "\n",
    "Students should submit their codes in the Python .ipynb. Each team needs to submit a “.zip” package including:\n",
    "- A “.csv” file containing the predicted labels for the test set (aka 2700_25). The file must only contain one column with each row being the prediction from your model based on the provided X, Y, and Z data.\n",
    "- A “.pdf” file for the report of your JupyterLab Notebook with all explanation for your data exploration, modeling, and bonus submission included.\n",
    "- A folder including all codes to execute the model. The JupyterLab notebook should run on it's own.\n",
    "- Bonus points up to 15 bonus HW points will be awarded to the group with the highest accuracy.\n",
    "    - 1st place = 15 pts\n",
    "    - 2nd place = 10 pts\n",
    "    - 3rd place = 5 pts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fa27db-569c-4274-a2b1-45134f82c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# .npy files are a much faster way of loading data from disk than using a CSV or TXT file \n",
    "# and are just as easy to work with as a CSV or TXT.\n",
    "\n",
    "# Example of loading a .npy file\n",
    "raw_data = np.load(\"x_1500_10.npy\")\n",
    "\n",
    "# Example of converting a numpy array to DataFrame\n",
    "df = pd.DataFrame(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f19931-4f4c-4622-9a36-a6c7526c4fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the first five lines to check the data imported correctly\n",
    "print(df.head())\n",
    "\n",
    "# Print out the size of the imported data again to check that it imported correctly \n",
    "# (the x_1500_10.npy file should contain 50,000 rows and 200 columns)\n",
    "print('Rows, Columns')\n",
    "print(df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
