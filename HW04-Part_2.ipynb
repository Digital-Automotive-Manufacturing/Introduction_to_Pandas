{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "232add90-809e-4bc2-b5c8-e9e2a947479f",
   "metadata": {},
   "source": [
    "# Introduction to Python Pandas Assignment\n",
    "\n",
    "Please use the included F1 dataset in the GitRepo\n",
    "\n",
    "If needed, the F1 Dataset Download Link is: https://www.kaggle.com/datasets/dubradave/formula-1-drivers-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d467e-da2d-4a20-b91e-f624cce6e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Import the dataset into a DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('F1DriversDataset.csv')\n",
    "\n",
    "# TODO Print the first 15 lines of the dataset\n",
    "\n",
    "\n",
    "\n",
    "# TODO Print all of the column names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098e7e2b-3ae5-4bc7-976e-d8d4f45f11e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Store all data from Lewis Hamilton in a new DataFrame df_LH\n",
    "\n",
    "\n",
    "\n",
    "# TODO Print out df_LH\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e172a58-fadf-485f-bbd3-01aec36e8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Store all data from Brazilian drivers in a new DataFrame df_Brazil using .loc\n",
    "\n",
    "\n",
    "\n",
    "# TODO Print out df_Brazil\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ab817-8a30-41a7-8482-b03f47e97b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Using sorting and then .iloc (cannot use .head()), sort the drivers by \n",
    "# Points and then print only the drivers with the 3rd-10th most points\n",
    "\n",
    "\n",
    "\n",
    "print(df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8638faa-965d-4b4f-854a-9aa01a3aad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Using sorting and then iloc (cannot use .head()), sort the drivers by \n",
    "# Points and then print only the drivers with the 3rd, 5th, and 9th most points\n",
    "\n",
    "\n",
    "\n",
    "print(df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09183564-4a5e-402b-bfa2-e0953c5cb34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Using loc and conditional statements print out only drivers from Italy with either:\n",
    "# 50 or more points \n",
    "# or \n",
    "# more than 200 race entries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04a5dfd-f1f5-4981-99a0-c53cf12ec376",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## Data Exploration\n",
    "\n",
    "For this next section, use the dataset files and description PDF. X, Y, and Z are the X, Y, and Z-axes respectively. gt is the Ground Truth file for the 1500 dataset.\n",
    "\n",
    "- The dataset includes vibration signals of normal and four different types of fault conditions and their corresponding ground-truth labels for two different operational conditions.\n",
    "- Vibration signals have been divided into smaller segments using window size of 200 data points to reduce the computational time.\n",
    "- Vibration signals are available in three directions (x, y, z). Participants are free to use either one direction or any combination of them.\n",
    "- The 1500_10 data will be used for training. You have x, y, and z data as well as the ground truth. \n",
    "- The 2700_25 data will be used for testing. You will only have the x, y, and z data but no ground truth.\n",
    "\n",
    "The dataset can be downloaded here (~457 MB required): https://drive.google.com/drive/folders/1L_V7W1cv3doRBaQl7MHbrPg3X0BlTc6u?usp=share_link\n",
    "\n",
    "Students should submit their codes in the Python .ipynb. Each team needs to submit a “.zip” package including:\n",
    "- A “.pdf” file for the report of your JupyterLab Notebook with all explanation for your data exploration included. Be sure to include at least:\n",
    "    - What does the data represent?\n",
    "    - What are the important data statistics you found?\n",
    "    - What cleaning and preperation are needed or did you complete?\n",
    "    - What can you tell about the dataset so far?\n",
    "    - What might your next steps to model the data be? You do not need to model it at this point, only provide an inital plan.\n",
    "    - Did you find any interesting information about the dataset?\n",
    "- A folder including all codes to execute the model and a “.pdf” file providing a brief and straightforward explanation on how to use the code and run the model.\n",
    "\n",
    "### BONUS #1 (<b><ins>Due April 3, 2023 11:59 PM EST</ins></b>)\n",
    "- Bonus points up to 10 bonus HW points will be awarded to the individual with the best and most creative data exploration. <ins>Think outside the box.</ins>\n",
    "    - 1st place = 10 pts\n",
    "    - 2nd place = 5 pts\n",
    "\n",
    "\n",
    "### BONUS #2 (<b><ins>Due separately April 10, 2023 11:59 PM EST</ins></b>)\n",
    "Students should submit their codes in the Python .ipynb. Each team needs to submit a “.zip” package including:\n",
    "- A “.csv” file containing the predicted labels for the test set (aka 2700_25). The file must only contain one column with each row being the prediction from your model based on the provided X, Y, and Z data.\n",
    "- A “.pdf” file for the report of your JupyterLab Notebook with all explanation for your data exploration and bonus submission included.\n",
    "- A folder including all codes to execute the model and a “.pdf” file providing a brief and straightforward explanation on how to use the code and run the model.\n",
    "- Bonus points up to 15 bonus HW points will be awarded to the group with the highest accuracy.\n",
    "    - 1st place = 15 pts\n",
    "    - 2nd place = 10 pts\n",
    "    - 3rd place = 5 pts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fa27db-569c-4274-a2b1-45134f82c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# .npy files are a much faster way of loading data from disk than using a CSV or TXT file \n",
    "# and are just as easy to work with as a CSV or TXT.\n",
    "\n",
    "# Example of loading a .npy file\n",
    "raw_data = np.load(\"x_1500_10.npy\")\n",
    "\n",
    "# Example of converting a numpy array to DataFrame\n",
    "df = pd.DataFrame(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f19931-4f4c-4622-9a36-a6c7526c4fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the first five lines to check the data imported correctly\n",
    "print(df.head())\n",
    "\n",
    "# Print out the size of the imported data again to check that it imported correctly \n",
    "# (the x_1500_10.npy file should contain 50,000 rows and 200 columns)\n",
    "print('Rows, Columns')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9266cc-5cd0-4da1-964d-30936a447534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9007fde-5005-4988-83fb-52500f9f6605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e67ecd-b3ba-4129-a2c8-5824925246ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd69ac95-8745-4947-813b-73a68c3a919c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d0ab5-78a6-48c4-a9da-64e6b6dd69c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e588c305-d49f-4881-8a62-c280e83f79b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a79628-9240-4ef6-9a86-74c630096b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ef083-9f96-4f5b-ae16-9747adc6fc66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56981d09-b34e-49ed-8e6c-9f1522a77b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c6641-1e88-46dc-917b-d0687c0e33b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596939b5-1608-43b4-a8c5-4848e9cda1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5e87d2-7be8-4b67-b05e-3d820e5fba59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ab2ba5-0c5e-4bf5-a92e-9acf67f39179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab48f7-ef33-4322-bc74-718d7ef6126f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e91d4a-5746-452e-8449-acc7ac4189e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
